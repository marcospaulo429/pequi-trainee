{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/AI-Core/Reinforcement-Learning/blob/master/Policy%20gradients%20solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, layers, embedding=False, distribution=False):\n",
    "        super().__init__()\n",
    "        l = []\n",
    "        for idx in range(len(layers) - 1):\n",
    "            l.append(torch.nn.Linear(layers[idx], layers[idx+1]))   # add a linear layer\n",
    "            if idx + 1 != len(layers) - 1: # if this is not the last layer ( +1 = zero indexed) (-1 = layer b4 last)\n",
    "                l.append(torch.nn.ReLU())   # activate\n",
    "        if distribution:    # if a probability dist output is required\n",
    "            l.append(torch.nn.Softmax())    # apply softmax to output\n",
    "            \n",
    "        self.layers = torch.nn.Sequential(*l) # unpack layers & turn into a function which applies them sequentially \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, optimiser, agent_tag, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "    assert not (use_baseline and use_causality)   # cant implement both simply\n",
    "    baseline = 0\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            avg_reward = 0\n",
    "            objective = 0\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                log_policy = []\n",
    "\n",
    "                rewards = []\n",
    "\n",
    "                step = 0\n",
    "\n",
    "                # RUN AN EPISODE\n",
    "                while not done:     # while the episode is not terminated\n",
    "                    state = torch.Tensor(state[0] if type(state) == tuple else state)     # correct data type for passing to model\n",
    "                    #print('STATE:', state)\n",
    "                    state = state.view(np.prod(state.shape))\n",
    "\n",
    "                    action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "                    #print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "                    action = int(action)\n",
    "                    # print('ACTION:', action)\n",
    "\n",
    "                    new_state, reward, done, _,info = env.step(action)    # take timestep\n",
    "\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    state = new_state\n",
    "                    log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "                    step += 1\n",
    "                    if done:\n",
    "                        break\n",
    "                    if step > 10000000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "                avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "                writer.add_scalar(f'{agent_tag}/Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "                # update baseline\n",
    "                if use_baseline:\n",
    "                    baseline += ( sum(rewards) - baseline ) / (epoch*episodes + episode + 1)    # accumulate average return  \n",
    "\n",
    "                for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "                    # add causality\n",
    "                    if use_causality:   \n",
    "                        weight = sum(rewards[idx:])     # only weight the log likelihood of this action by the future rewards, not the total\n",
    "                    else:\n",
    "                        weight = sum(rewards) - baseline           # weight by the total reward from this episode\n",
    "                    objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "            objective /= episodes   # average over episodes\n",
    "            objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "            # UPDATE POLICY\n",
    "            # print('updating policy')\n",
    "            print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "            objective.backward()    # backprop\n",
    "            optimiser.step()    # update params\n",
    "            optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "            # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                state = torch.Tensor(state[0] if type(state) == tuple else state)\n",
    "                state = state.view(np.prod(state.shape))\n",
    "                action_distribution = policy(state)\n",
    "                action = torch.distributions.Categorical(action_distribution).sample()\n",
    "                action = int(action)\n",
    "                state, reward, done,_ ,info = env.step(action)\n",
    "                sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        env.close()\n",
    "\n",
    "    env.close()\n",
    "    checkpoint = {\n",
    "        'model': policy,\n",
    "        'state_dict': policy.state_dict() \n",
    "    }\n",
    "    torch.save(checkpoint, f\"reinforce_agents/trained-agent-{agent_tag}.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcospaulo/Documents/CEIA/Reforço/Reinforcement-Learning-AKCIT/Implementation of Algorithms/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "/Users/marcospaulo/Documents/CEIA/Reforço/Reinforcement-Learning-AKCIT/Implementation of Algorithms/env/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:250: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 23.00\n",
      "EPOCH: 1 AVG REWARD: 25.20\n",
      "EPOCH: 2 AVG REWARD: 25.73\n",
      "EPOCH: 3 AVG REWARD: 25.60\n",
      "EPOCH: 4 AVG REWARD: 24.70\n",
      "EPOCH: 5 AVG REWARD: 30.07\n",
      "EPOCH: 6 AVG REWARD: 29.53\n",
      "EPOCH: 7 AVG REWARD: 24.17\n",
      "EPOCH: 8 AVG REWARD: 27.73\n",
      "EPOCH: 9 AVG REWARD: 27.83\n",
      "EPOCH: 10 AVG REWARD: 25.00\n",
      "EPOCH: 11 AVG REWARD: 30.87\n",
      "EPOCH: 12 AVG REWARD: 31.67\n",
      "EPOCH: 13 AVG REWARD: 32.03\n",
      "EPOCH: 14 AVG REWARD: 26.83\n",
      "EPOCH: 15 AVG REWARD: 28.70\n",
      "EPOCH: 16 AVG REWARD: 25.83\n",
      "EPOCH: 17 AVG REWARD: 36.83\n",
      "EPOCH: 18 AVG REWARD: 29.53\n",
      "EPOCH: 19 AVG REWARD: 31.93\n",
      "EPOCH: 20 AVG REWARD: 30.77\n",
      "EPOCH: 21 AVG REWARD: 35.43\n",
      "EPOCH: 22 AVG REWARD: 28.67\n",
      "EPOCH: 23 AVG REWARD: 39.40\n",
      "EPOCH: 24 AVG REWARD: 34.90\n",
      "EPOCH: 25 AVG REWARD: 43.67\n",
      "EPOCH: 26 AVG REWARD: 36.30\n",
      "EPOCH: 27 AVG REWARD: 35.37\n",
      "EPOCH: 28 AVG REWARD: 37.67\n",
      "EPOCH: 29 AVG REWARD: 45.90\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'cartpole-improved'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 43.17\n",
      "EPOCH: 1 AVG REWARD: 46.93\n",
      "EPOCH: 2 AVG REWARD: 49.53\n",
      "EPOCH: 3 AVG REWARD: 38.33\n",
      "EPOCH: 4 AVG REWARD: 48.13\n",
      "EPOCH: 5 AVG REWARD: 50.97\n",
      "EPOCH: 6 AVG REWARD: 48.53\n",
      "EPOCH: 7 AVG REWARD: 52.70\n",
      "EPOCH: 8 AVG REWARD: 52.30\n",
      "EPOCH: 9 AVG REWARD: 55.83\n",
      "EPOCH: 10 AVG REWARD: 52.17\n",
      "EPOCH: 11 AVG REWARD: 48.30\n",
      "EPOCH: 12 AVG REWARD: 64.60\n",
      "EPOCH: 13 AVG REWARD: 71.97\n",
      "EPOCH: 14 AVG REWARD: 67.97\n",
      "EPOCH: 15 AVG REWARD: 68.87\n",
      "EPOCH: 16 AVG REWARD: 46.20\n",
      "EPOCH: 17 AVG REWARD: 67.30\n",
      "EPOCH: 18 AVG REWARD: 77.00\n",
      "EPOCH: 19 AVG REWARD: 66.73\n",
      "EPOCH: 20 AVG REWARD: 25.50\n",
      "EPOCH: 21 AVG REWARD: 29.17\n",
      "EPOCH: 22 AVG REWARD: 33.20\n",
      "EPOCH: 23 AVG REWARD: 40.50\n",
      "EPOCH: 24 AVG REWARD: 55.23\n",
      "EPOCH: 25 AVG REWARD: 121.17\n",
      "EPOCH: 26 AVG REWARD: 67.13\n",
      "EPOCH: 27 AVG REWARD: 109.23\n",
      "EPOCH: 28 AVG REWARD: 64.87\n",
      "EPOCH: 29 AVG REWARD: 127.60\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=False,\n",
    "    use_causality=True,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hl/vnf72w053hx2r08wdgsn_v7c0000gn/T/ipykernel_9799/4198290732.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cartpole_agent_params = torch.load('reinforce_agents/trained-agent-cartpole-improved.pt')\n",
      "/Users/marcospaulo/Documents/CEIA/Reforço/Reinforcement-Learning-AKCIT/Implementation of Algorithms/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def deploy(env, saved_model):\n",
    "    \n",
    "#     policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True) # we must remember the architecture\n",
    "    policy = saved_model['model']\n",
    "    policy.load_state_dict(saved_model['state_dict']) # load in our pre-trained model\n",
    "    policy.eval() # put our model in evaluation mode\n",
    "    try:\n",
    "        for episode in range(100): # keep demonstrating your skills\n",
    "                done = False # not done yet\n",
    "                observation = env.reset() # initialise the environemt\n",
    "                while not done: # until the episode is over\n",
    "                    observation = torch.Tensor(observation[0] if type(observation) == tuple else observation) # turn observation to tensor\n",
    "                    observation = observation.view(np.prod(observation.shape)) # view observation as vector\n",
    "                    action_distribution = policy(observation) # infer what actions to take with what probability\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample() # sample an action from that distribution\n",
    "                    action = int(action) # make it an int not a float\n",
    "                    observation, reward, done, _,info = env.step(action) # take an action and transition the environment\n",
    "                    env.render() # show us the environment\n",
    "                    sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "       \n",
    "\n",
    "cartpole_env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "cartpole_agent_params = torch.load('reinforce_agents/trained-agent-cartpole-improved.pt')\n",
    "deploy(cartpole_env, cartpole_agent_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
