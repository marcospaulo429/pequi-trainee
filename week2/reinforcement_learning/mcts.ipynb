{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Criando o ambiente FrozenLake-v0 com is_slippery=False\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: rodar política base para obter D(s,a,s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_dataset(env, num_episodes):\n",
    "    \"\"\"\n",
    "    Gera um conjunto de dados estruturado em episódios.\n",
    "\n",
    "    Args:\n",
    "        env: O ambiente OpenAI Gym.\n",
    "        num_episodes: Número de episódios a serem gerados.\n",
    "\n",
    "    Returns:\n",
    "        episodes: Lista de episódios, onde cada episódio é uma lista de transições.\n",
    "    \"\"\"\n",
    "    episodes = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (done or truncated):\n",
    "            action = env.action_space.sample()  # Ação aleatória\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "            if type(state) == tuple:  # Tratando estados que podem ser tuplas\n",
    "                state = state[0]\n",
    "\n",
    "            # Adicionar a transição ao episódio\n",
    "            episode.append((state, int(action), reward, next_state))\n",
    "            state = next_state\n",
    "\n",
    "        # Adicionar o episódio à lista de episódios\n",
    "        episodes.append(episode)\n",
    "\n",
    "    return episodes\n",
    "\n",
    "num_episodes = 100\n",
    "episodes = generate_dataset(env, num_episodes)\n",
    "\n",
    "# Exibindo informações sobre o conjunto de dados\n",
    "print(f\"Total de episódios gerados: {len(episodes)}\")\n",
    "print(f\"Exemplo de transição no primeiro episódio: {episodes[0][1]}\")  # Primeira transição do primeiro episódio\n",
    "print(f\"Tamanho do primeiro episódio: {len(episodes[0])}\")\n",
    "print(f\"Exemplo de episódio: {episodes[0]}\")\n",
    "\n",
    "# Fechando o ambiente\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: aprender o modelo de RL para podemos prever as dinâmicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def prepare_dataset(episodes):\n",
    "    \"\"\"\n",
    "    Prepara os dados para o modelo de transição.\n",
    "\n",
    "    Args:\n",
    "        episodes: Lista de episódios, onde cada episódio é uma lista de transições.\n",
    "\n",
    "    Returns:\n",
    "        X: Array de features (state, action).\n",
    "        y: Array de alvos (next_state).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for episode in episodes:\n",
    "        for transition in episode:\n",
    "            state, action, reward, next_state = transition\n",
    "            # Concatenar estado e ação como features\n",
    "            X.append([state, action])\n",
    "            # Definir next_state como alvo\n",
    "            y.append(next_state)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_dataset(episodes)\n",
    "\n",
    "# Dividir o dataset em treino e teste usando os dados codificados\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# ----------------------- Treinamento do Modelo de Mundo -----------------------\n",
    "\n",
    "# Treinar o modelo de Random Forest\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prever e calcular a acurácia no conjunto de treino\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Acurácia no dataset de treino: {train_accuracy:.4f}\")\n",
    "\n",
    "# Prever e calcular a acurácia no conjunto de teste\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Acurácia no dataset de teste: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Fazer planning com nosso modelo de mundo\n",
    "\n",
    "Implementação baseado em : https://ai-boson.github.io/mcts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MonteCarloTreeSearchNode:\n",
    "    def __init__(self, state, parent=None, parent_action=None, model_world=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.model_world = model_world\n",
    "\n",
    "        self.children = []\n",
    "        self._number_of_visits = 0\n",
    "        self._results = defaultdict(int)\n",
    "        # Para simplificar, vamos contar vitórias (+1) e derrotas (-1).\n",
    "        self._results[1] = 0\n",
    "        self._results[-1] = 0\n",
    "        \n",
    "        self._untried_actions = self.untried_actions()\n",
    "\n",
    "    def untried_actions(self):\n",
    "        \"\"\"\n",
    "        Retorna as ações não tentadas. No FrozenLake, são 4 ações possíveis: 0,1,2,3.\n",
    "        Aqui simplesmente retornamos todas, mas poderíamos personalizar.\n",
    "        \"\"\"\n",
    "        return [0, 1, 2, 3]\n",
    "\n",
    "    def q(self):\n",
    "        \"\"\"Retorna a diferença (vitórias - derrotas)\"\"\"\n",
    "        return self._results[1] - self._results[-1]\n",
    "\n",
    "    def n(self):\n",
    "        \"\"\"Retorna o número de visitas a este nó.\"\"\"\n",
    "        return self._number_of_visits\n",
    "\n",
    "    def expand(self):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação ainda não tentada, prediz o próximo estado via modelo de mundo\n",
    "        e cria um novo nó filho.\n",
    "        \"\"\"\n",
    "        action = self._untried_actions.pop()\n",
    "        next_state = self.model_world.predict([[self.state, action]])[0]\n",
    "        \n",
    "        child_node = MonteCarloTreeSearchNode(\n",
    "            state=next_state,\n",
    "            parent=self,\n",
    "            parent_action=action,\n",
    "            model_world=self.model_world\n",
    "        )\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def is_terminal_node(self):\n",
    "        \"\"\"\n",
    "        Condição de parada. No FrozenLake, consideramos terminal se for buraco ou meta.\n",
    "        \"\"\"\n",
    "        return self.state in [5, 7, 11, 12, 15]  # buracos e meta\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        Simula até o fim (jogando ações aleatórias) e retorna +1 se caiu na meta,\n",
    "        -1 se caiu no buraco, ou 0 se não se sabe (mas aqui sempre termina).\n",
    "        \"\"\"\n",
    "        current_rollout_state = self\n",
    "        \n",
    "        while not current_rollout_state.is_terminal_node():\n",
    "            possible_moves = current_rollout_state.get_legal_actions()\n",
    "            action = self.rollout_policy(possible_moves)\n",
    "            current_rollout_state = current_rollout_state.move(action)\n",
    "        return current_rollout_state.game_result()\n",
    "\n",
    "    def rollout_policy(self, possible_moves):\n",
    "        \"\"\"\n",
    "        Política de rollout (pode ser aleatória). Aqui escolhemos ação uniformemente.\n",
    "        \"\"\"\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"\n",
    "        Se não há mais ações a explorar a partir deste nó, consideramos 'fully expanded'.\n",
    "        \"\"\"\n",
    "        return len(self._untried_actions) == 0\n",
    "\n",
    "    def best_child(self, c_param=0.1):\n",
    "        \"\"\"\n",
    "        Escolhe o melhor filho usando Upper Confidence Bound (UCB1).\n",
    "        \"\"\"\n",
    "        choices_weights = [\n",
    "            (c.q() / (c.n() + 1e-6)) + c_param * np.sqrt(2 * np.log(self.n() + 1e-6) / (c.n() + 1e-6))\n",
    "            for c in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def _tree_policy(self):\n",
    "        \"\"\"\n",
    "        Estratégia para descer na árvore:\n",
    "        - Se não estiver totalmente expandido, expandir.\n",
    "        - Senão, escolhe o melhor filho e continua.\n",
    "        \"\"\"\n",
    "        current_node = self\n",
    "        while not current_node.is_terminal_node():\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n",
    "\n",
    "    def best_action(self, simulation_no=100):\n",
    "        \"\"\"\n",
    "        Realiza várias simulações (rollouts). \n",
    "        Depois de simulações, devolve a ação do melhor filho (c_param=0) para exploração=0.\n",
    "        \"\"\"\n",
    "        for _ in range(simulation_no):\n",
    "            v = self._tree_policy()\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "\n",
    "        best_child_node = self.best_child(c_param=0.0)\n",
    "        return best_child_node.parent_action\n",
    "\n",
    "    def backpropagate(self, result):\n",
    "        \"\"\"\n",
    "        Atualiza as estatísticas (visitas, vitórias/derrotas) e sobe para o pai.\n",
    "        \"\"\"\n",
    "        self._number_of_visits += 1\n",
    "        if result == 1:\n",
    "            self._results[1] += 1\n",
    "        elif result == -1:\n",
    "            self._results[-1] += 1\n",
    "        \n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(result)\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        \"\"\"\n",
    "        No FrozenLake, normalmente temos 4 ações (0,1,2,3). \n",
    "        \"\"\"\n",
    "        return [0, 1, 2, 3]\n",
    "\n",
    "    def is_game_over(self):\n",
    "        \"\"\"\n",
    "        Verifica se estado é terminal (buraco ou meta).\n",
    "        \"\"\"\n",
    "        return self.is_terminal_node()\n",
    "\n",
    "    def game_result(self):\n",
    "        \"\"\"\n",
    "        Se estado é meta (15), retorna +1; se é buraco, retorna -1; senão 0.\n",
    "        \"\"\"\n",
    "        if self.state == 15:\n",
    "            return 1\n",
    "        elif self.state in [5, 7, 11, 12]:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        Executa um 'passo' no modelo para obter próximo estado\n",
    "        e devolve um novo nó (para simplificar o rollout).\n",
    "        \"\"\"\n",
    "        next_state = self.model_world.predict([[self.state, action]])[0]\n",
    "        return MonteCarloTreeSearchNode(next_state, model_world=self.model_world)\n",
    "\n",
    "\n",
    "def MPC_with_MCTS(env, model, num_episodes=10, num_simulations=100, rollout_depth=10):\n",
    "    \"\"\"\n",
    "    Aplica MCTS com o modelo de mundo no FrozenLake.\n",
    "\n",
    "    Args:\n",
    "        env: O ambiente OpenAI Gym.\n",
    "        model: O modelo de mundo treinado.\n",
    "        num_episodes: Número de episódios para executar.\n",
    "        num_simulations: Número de simulações por decisão de ação.\n",
    "        rollout_depth: Profundidade do rollout durante a simulação.\n",
    "\n",
    "    Returns:\n",
    "        episodes: Lista de episódios gerados.\n",
    "    \"\"\"\n",
    "    episodes = []\n",
    "\n",
    "    for episode_num in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = int(state)\n",
    "        episode = []\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        while not (done or truncated):\n",
    "            # Cria nó raiz para este estado e obtém a melhor ação via MCTS\n",
    "            root = MonteCarloTreeSearchNode(state=state, model_world=model)\n",
    "            action = root.best_action(simulation_no=num_simulations)\n",
    "            \n",
    "            # Executa a ação real no ambiente\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            next_state = int(next_state)\n",
    "            reward = float(reward)\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"Ep {episode_num + 1}, Ação: {action}, Recompensa: {reward}, Estado: {state}, Próx Estado: {next_state}\")\n",
    "\n",
    "            # Adiciona transição ao episódio\n",
    "            episode.append((state, int(action), reward, next_state, total_reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Salva o episódio completo\n",
    "        episodes.append(episode)\n",
    "\n",
    "    return episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mcts_episodes = 50\n",
    "num_simulations_per_decision = 100\n",
    "rollout_depth = 250\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "mcts_episodes = MPC_with_MCTS(env, model, \n",
    "                              num_episodes=num_mcts_episodes, \n",
    "                              num_simulations=num_simulations_per_decision, \n",
    "                              rollout_depth=rollout_depth)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Total de episódios gerados pelo MCTS: {len(mcts_episodes)}\")\n",
    "print(f\"Exemplo de transição no primeiro episódio gerado pelo MCTS: {mcts_episodes[0][0]}\")\n",
    "print(f\"Tamanho do primeiro episódio gerado pelo MCTS: {len(mcts_episodes[0])}\")\n",
    "print(f\"Exemplo de episódio gerado pelo MCTS: {mcts_episodes[0]}\")\n",
    "\n",
    "total_rewards = []\n",
    "for ep in mcts_episodes:\n",
    "    episode_reward = sum(transition[2] for transition in ep)\n",
    "    total_rewards.append(episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(total_rewards) + 1), total_rewards, marker='o')\n",
    "plt.xlabel(\"Época (Episódio)\")\n",
    "plt.ylabel(\"Recompensa Total Acumulada\")\n",
    "plt.title(\"Recompensa x Épocas (MCTS)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mcts_episodes = 50\n",
    "num_simulations_per_decision = 100\n",
    "rollout_depth = 150\n",
    "\n",
    "# Cria novamente o ambiente (ou reabre) para executar\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "mcts_episodes = MPC_with_MCTS(env, model, \n",
    "                              num_episodes=num_mcts_episodes, \n",
    "                              num_simulations=num_simulations_per_decision, \n",
    "                              rollout_depth=rollout_depth)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Total de episódios gerados pelo MCTS: {len(mcts_episodes)}\")\n",
    "print(f\"Exemplo de transição no primeiro episódio gerado pelo MCTS: {mcts_episodes[0][0]}\")\n",
    "print(f\"Tamanho do primeiro episódio gerado pelo MCTS: {len(mcts_episodes[0])}\")\n",
    "print(f\"Exemplo de episódio gerado pelo MCTS: {mcts_episodes[0]}\")\n",
    "\n",
    "total_rewards = []\n",
    "for ep in mcts_episodes:\n",
    "    episode_reward = sum(transition[2] for transition in ep)\n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(total_rewards) + 1), total_rewards, marker='o')\n",
    "plt.xlabel(\"Época (Episódio)\")\n",
    "plt.ylabel(\"Recompensa Total Acumulada\")\n",
    "plt.title(\"Recompensa x Épocas (MCTS)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
